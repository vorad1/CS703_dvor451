{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vorad1/CS703_dvor451/blob/working/Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWgOjPEQOrMz"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "HF_TOKEN=userdata.get('HF_TOKEN')\n",
        "\n",
        "from huggingface_hub import login\n",
        "import os\n",
        "\n",
        "# Get the secret\n",
        "token = HF_TOKEN\n",
        "login(token=token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxIlxeSiUWg0"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade datasets\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Force re-download by setting download_mode and clearing cache dir\n",
        "dataset = load_dataset(\n",
        "    \"commonsense_qa\",\n",
        "    download_mode=\"force_redownload\",\n",
        "    cache_dir=\"/content/temp_cache\"\n",
        ")\n",
        "\n",
        "!pip install -U datasets\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "\n",
        "# Load model and tokenizer\n",
        "model_name = \"google/flan-t5-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Set up pipeline\n",
        "pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Load dataset with custom cache directory\n",
        "# dataset = load_dataset(\"commonsense_qa\", cache_dir=\"/content/hf_cache\")\n",
        "\n",
        "# Selecting 2 in-context examples and 1 test question\n",
        "prompt = \"\"\"Question: Why would someone wear gloves?\n",
        "A) To keep their hands clean\n",
        "B) To make their hands soft\n",
        "C) To show off\n",
        "D) To cover scars\n",
        "Answer: A\n",
        "\n",
        "Question: Why do people use umbrellas?\n",
        "A) To fly\n",
        "B) To block the sun or rain\n",
        "C) To walk faster\n",
        "D) To play with\n",
        "Answer: B\n",
        "\n",
        "Question: Why do people wear sunglasses?\n",
        "A) To look cool\n",
        "B) To protect their eyes from bright light\n",
        "C) To see better in the dark\n",
        "D) To avoid being recognized\n",
        "Answer:\"\"\"\n",
        "\n",
        "# Generate prediction based on training\n",
        "result = pipe(prompt, max_new_tokens=5)[0]['generated_text']\n",
        "print(\"Model's answer:\", result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvWjrwuGwzLc"
      },
      "outputs": [],
      "source": [
        "!pip install -U datasets\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Loading FLAN-T5 model and tokenizer\n",
        "model_name = \"google/flan-t5-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Set up inference pipeline\n",
        "pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "#load dataset with a smaller split for testing\n",
        "dataset = load_dataset(\"commonsense_qa\", split=\"validation[:50]\")\n",
        "\n",
        "\n",
        "import random\n",
        "\n",
        "# using 2 questions from the training set\n",
        "context_examples = [\n",
        "  {\n",
        "    \"question\": \"Why would someone wear gloves?\",\n",
        "    \"choices\": [\"To keep their hands clean\", \"To make their hands soft\", \"To show off\", \"To cover scars\"],\n",
        "    \"answer\": \"A\"\n",
        "  },\n",
        "  {\n",
        "    \"question\": \"Why do people use umbrellas?\",\n",
        "    \"choices\": [\"To fly\", \"To block the sun or rain\", \"To walk faster\", \"To play with\"],\n",
        "    \"answer\": \"B\"\n",
        "  }\n",
        "]\n",
        "\n",
        "# mapping the letter to create a functions of each letter\n",
        "letter_map = [\"A\",\"B\",\"C\",\"D\",\"E\"]\n",
        "\n",
        "# defining variables to keep track of the score\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "for sample in dataset:\n",
        "    question = sample[\"question\"]\n",
        "    choices = sample[\"choices\"][\"text\"]\n",
        "    correct_answer_idx = sample[\"answerKey\"]\n",
        "    correct_answer = correct_answer_idx\n",
        "\n",
        "    #Building the in-context prompt\n",
        "    prompt = \"\"\n",
        "    for ex in context_examples:\n",
        "        ex_q = ex[\"question\"]\n",
        "        ex_choices = ex[\"choices\"]\n",
        "        ex_ans = ex[\"answer\"]\n",
        "        prompt += f\"Question: {ex_q}\\n\"\n",
        "        for i, choice in enumerate(ex_choices):\n",
        "            prompt += f\"{letter_map[i]}) {choice}\\n\"\n",
        "        prompt += f\"Answer: {ex_ans}\\n\\n\"\n",
        "\n",
        "\n",
        "    prompt += f\"Question: {question}\\n\"\n",
        "    for i, choice in enumerate(choices):\n",
        "        prompt += f\"{letter_map[i]}) {choice}\\n\"\n",
        "    prompt += \"Answer:\"\n",
        "\n",
        "    # generating the answer\n",
        "    result = pipe(prompt, max_new_tokens=5, do_sample=False)[0]['generated_text'].strip()\n",
        "    prediction = result[0] if result and result[0] in letter_map else \"?\"\n",
        "\n",
        "    # comparing with the truth\n",
        "    is_correct = (prediction == correct_answer)\n",
        "    correct += int(is_correct)\n",
        "    total += 1\n",
        "\n",
        "    print(f\"Q: {question}\")\n",
        "    print(f\"Predicted: {prediction}, Actual: {correct_answer}, Correct: {is_correct}\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f\"\\nFinal Accuracy over {total} examples: {accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56H9tW1oVqTD"
      },
      "outputs": [],
      "source": [
        "!pip install -U datasets\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Loading FLAN-T5 model and tokenizer\n",
        "model_name = \"google/flan-t5-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Set up inference pipeline\n",
        "pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "#load dataset with a smaller split for testing\n",
        "dataset = load_dataset(\"commonsense_qa\", split=\"validation[:50]\")\n",
        "\n",
        "\n",
        "import random\n",
        "\n",
        "# using 2 questions from the training set\n",
        "context_examples = [\n",
        "  {\n",
        "    \"question\": \"Why would someone wear gloves?\",\n",
        "    \"choices\": [\"To keep their hands clean\", \"To make their hands soft\", \"To show off\", \"To cover scars\"],\n",
        "    \"answer\": \"A\"\n",
        "  },\n",
        "  {\n",
        "    \"question\": \"Why do people use umbrellas?\",\n",
        "    \"choices\": [\"To fly\", \"To block the sun or rain\", \"To walk faster\", \"To play with\"],\n",
        "    \"answer\": \"B\"\n",
        "  }\n",
        "]\n",
        "\n",
        "# mapping the letter to create a functions of each letter\n",
        "letter_map = [\"A\",\"B\",\"C\",\"D\",\"E\"]\n",
        "\n",
        "# defining variables to keep track of the score\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "for sample in dataset:\n",
        "    question = sample[\"question\"]\n",
        "    choices = sample[\"choices\"][\"text\"]\n",
        "    correct_answer_idx = sample[\"answerKey\"]\n",
        "    correct_answer = correct_answer_idx\n",
        "\n",
        "    #Building the in-context prompt\n",
        "    prompt = \"\"\n",
        "    for ex in context_examples:\n",
        "        ex_q = ex[\"question\"]\n",
        "        ex_choices = ex[\"choices\"]\n",
        "        ex_ans = ex[\"answer\"]\n",
        "        prompt += f\"Question: {ex_q}\\n\"\n",
        "        for i, choice in enumerate(ex_choices):\n",
        "            prompt += f\"{letter_map[i]}) {choice}\\n\"\n",
        "        prompt += f\"Answer: {ex_ans}\\n\\n\"\n",
        "\n",
        "\n",
        "    prompt += f\"Question: {question}\\n\"\n",
        "    for i, choice in enumerate(choices):\n",
        "        prompt += f\"{letter_map[i]}) {choice}\\n\"\n",
        "    prompt += \"Answer:\"\n",
        "\n",
        "    # generating the answer\n",
        "    result = pipe(prompt, max_new_tokens=5, do_sample=False)[0]['generated_text'].strip()\n",
        "    prediction = result[0] if result and result[0] in letter_map else \"?\"\n",
        "\n",
        "    # comparing with the truth\n",
        "    is_correct = (prediction == correct_answer)\n",
        "    correct += int(is_correct)\n",
        "    total += 1\n",
        "\n",
        "    print(f\"Q: {question}\")\n",
        "    print(f\"Predicted: {prediction}, Actual: {correct_answer}, Correct: {is_correct}\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f\"\\nFinal Accuracy over {total} examples: {accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ei2M00k2M7b3"
      },
      "outputs": [],
      "source": [
        "!pip install -U datasets\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Loading FLAN-T5 model and tokenizer\n",
        "model_name = \"google/flan-t5-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Set up inference pipeline\n",
        "pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "#load dataset with a smaller split for testing\n",
        "dataset = load_dataset(\"commonsense_qa\", split=\"validation[:50]\")\n",
        "\n",
        "import csv\n",
        "import os\n",
        "\n",
        "# Create a folder to store results (optional)\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "\n",
        "# **Define `context_examples` and `letter_map` here, before they are used:**\n",
        "context_examples = [\n",
        "  {\n",
        "    \"question\": \"Why would someone wear gloves?\",\n",
        "    \"choices\": [\"To keep their hands clean\", \"To make their hands soft\", \"To show off\", \"To cover scars\"],\n",
        "    \"answer\": \"A\"\n",
        "  },\n",
        "  {\n",
        "    \"question\": \"Why do people use umbrellas?\",\n",
        "    \"choices\": [\"To fly\", \"To block the sun or rain\", \"To walk faster\", \"To play with\"],\n",
        "    \"answer\": \"B\"\n",
        "  }\n",
        "]\n",
        "\n",
        "letter_map = [\"A\",\"B\",\"C\",\"D\",\"E\"]\n",
        "!pip install -U datasets\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Loading FLAN-T5 model and tokenizer\n",
        "model_name = \"google/flan-t5-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Set up inference pipeline\n",
        "pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "#load dataset with a smaller split for testing\n",
        "dataset = load_dataset(\"commonsense_qa\", split=\"validation[:50]\")\n",
        "\n",
        "import csv\n",
        "import os\n",
        "\n",
        "# Create a folder to store results (optional)\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "\n",
        "# **Define `context_examples` and `letter_map` here, before they are used:**\n",
        "context_examples = [\n",
        "  {\n",
        "    \"question\": \"Why would someone wear gloves?\",\n",
        "    \"choices\": [\"To keep their hands clean\", \"To make their hands soft\", \"To show off\", \"To cover scars\"],\n",
        "    \"answer\": \"A\"\n",
        "  },\n",
        "  {\n",
        "    \"question\": \"Why do people use umbrellas?\",\n",
        "    \"choices\": [\"To fly\", \"To block the sun or rain\", \"To walk faster\", \"To play with\"],\n",
        "    \"answer\": \"B\"\n",
        "  }\n",
        "]\n",
        "\n",
        "letter_map = [\"A\",\"B\",\"C\",\"D\",\"E\"]\n",
        "\n",
        "# Open a CSV file to save results\n",
        "with open(\"C:\\\\Users\\\\chris\\\\Downloads\\\\results\\\\icl_results.csv\", mode=\"w\", newline=\"\") as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"Question\", \"Choices\", \"GroundTruth\", \"Prediction\", \"Correct\"])\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for sample in dataset:\n",
        "        question = sample[\"question\"]\n",
        "        choices = sample[\"choices\"][\"text\"]\n",
        "        correct_answer_idx = sample[\"answerKey\"]\n",
        "        correct_answer = correct_answer_idx\n",
        "\n",
        "        # Build in-context prompt\n",
        "        prompt = \"\"\n",
        "        for ex in context_examples:\n",
        "            ex_q = ex[\"question\"]\n",
        "            ex_choices = ex[\"choices\"]\n",
        "            ex_ans = ex[\"answer\"]\n",
        "            prompt += f\"Question: {ex_q}\\n\"\n",
        "            for i, c in enumerate(ex_choices):\n",
        "                prompt += f\"{letter_map[i]}) {c}\\n\"\n",
        "            prompt += f\"Answer: {ex_ans}\\n\\n\"\n",
        "\n",
        "        prompt += f\"Question: {question}\\n\"\n",
        "        for i, c in enumerate(choices):\n",
        "            prompt += f\"{letter_map[i]}) {c}\\n\"\n",
        "        prompt += \"Answer:\"\n",
        "\n",
        "        # Generate answer\n",
        "        result = pipe(prompt, max_new_tokens=5, do_sample=False)[0]['generated_text'].strip()\n",
        "        prediction = result[0] if result and result[0] in letter_map else \"?\"\n",
        "\n",
        "        is_correct = (prediction == correct_answer)\n",
        "        correct += int(is_correct)\n",
        "        total += 1\n",
        "\n",
        "        # Write row to CSV\n",
        "        writer.writerow([\n",
        "            question,\n",
        "            \" | \".join(choices),\n",
        "            correct_answer,\n",
        "            prediction,\n",
        "            \"Yes\" if is_correct else \"No\"\n",
        "        ])\n",
        "\n",
        "        print(f\"Processed Q{total}: {prediction} vs. {correct_answer} â€” {'Correct' if is_correct else 'Wrong'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvHtFtx7IfVe"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets\n",
        "\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from datasets import load_dataset\n",
        "import random\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model_name = \"google/flan-t5-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Load and sample CommonsenseQA dataset\n",
        "dataset = load_dataset(\"commonsense_qa\", split=\"validation[:100]\")\n",
        "\n",
        "# Define the prompt styles\n",
        "def format_zero_shot(question, _=None):\n",
        "    return f\"Answer the question: {question}\"\n",
        "\n",
        "def format_few_shot(question, few_examples):\n",
        "    prompt = \"\"\n",
        "    for ex in few_examples:\n",
        "        prompt += f\"Q: {ex['question']}\\nA: {ex['answerKey']}\\n\\n\"\n",
        "    prompt += f\"Q: {question}\\nA:\"\n",
        "    return prompt\n",
        "\n",
        "def format_natural_prompt(question, few_examples):\n",
        "    prompt = \"Here are some commonsense questions with answers:\\n\"\n",
        "    for ex in few_examples:\n",
        "        prompt += f\"Someone asked: {ex['question']}\\nThe answer is: {ex['answerKey']}\\n\"\n",
        "    prompt += f\"Now answer this: {question}\\nAnswer:\"\n",
        "    return prompt\n",
        "\n",
        "# Choose few-shot examples\n",
        "few_shot_examples = random.sample(list(dataset), 5)\n",
        "\n",
        "# Define configurations\n",
        "configs = {\n",
        "    \"zero-shot\": {\"shot\": 0, \"format_func\": format_zero_shot},\n",
        "    \"3-shot\": {\"shot\": 3, \"format_func\": lambda q, exs: format_few_shot(q, exs[:3])},\n",
        "    \"5-shot\": {\"shot\": 5, \"format_func\": lambda q, exs: format_few_shot(q, exs)},\n",
        "    \"naturalistic\": {\"shot\": 3, \"format_func\": lambda q, exs: format_natural_prompt(q, exs[:3])}\n",
        "}\n",
        "\n",
        "results = []\n",
        "\n",
        "for config_name, config in configs.items():\n",
        "    correct = 0\n",
        "    print(f\"\\nRunning config: {config_name}\")\n",
        "    for sample in tqdm(dataset):\n",
        "        prompt = config[\"format_func\"](sample[\"question\"], few_shot_examples)\n",
        "        output = pipe(prompt, max_new_tokens=10)[0][\"generated_text\"].strip()\n",
        "        if sample[\"answerKey\"].lower() in output.lower():\n",
        "            correct += 1\n",
        "    accuracy = correct / len(dataset) * 100\n",
        "    results.append({\"Configuration\": config_name, \"Accuracy (%)\": round(accuracy, 2)})\n",
        "\n",
        "\n",
        "# Save results to CSV\n",
        "df_results = pd.DataFrame(results)\n",
        "df_results.to_csv(\"accuracy_results.csv\", index=False)\n",
        "\n",
        "# Display results\n",
        "df_results\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPEmY643kSNg0DOVifxQSZ6",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}