{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMlcJk3NYwWRYBDyI1fmUu+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vorad1/CS703_dvor451/blob/working/Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "HF_TOKEN=userdata.get('HF_TOKEN')\n",
        "\n",
        "from huggingface_hub import login\n",
        "import os\n",
        "\n",
        "# Get the secret\n",
        "token = HF_TOKEN\n",
        "login(token=token)"
      ],
      "metadata": {
        "id": "hWgOjPEQOrMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Loading FLAN-T5 model and tokenizer\n",
        "model_name = \"google/flan-t5-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Set up inference pipeline\n",
        "pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Load CommonsenseQA dataset\n",
        "dataset = load_dataset(\"commonsense_qa\", cache_dir=\"/content/hf_cache\")\n",
        "\n",
        "# Selecting 2 in-context examples and 1 test question\n",
        "prompt = \"\"\"Question: Why would someone wear gloves?\n",
        "A) To keep their hands clean\n",
        "B) To make their hands soft\n",
        "C) To show off\n",
        "D) To cover scars\n",
        "Answer: A\n",
        "\n",
        "Question: Why do people use umbrellas?\n",
        "A) To fly\n",
        "B) To block the sun or rain\n",
        "C) To walk faster\n",
        "D) To play with\n",
        "Answer: B\n",
        "\n",
        "Question: Why do people wear sunglasses?\n",
        "A) To look cool\n",
        "B) To protect their eyes from bright light\n",
        "C) To see better in the dark\n",
        "D) To avoid being recognized\n",
        "Answer:\"\"\"\n",
        "\n",
        "# Generate prediction based on training\n",
        "result = pipe(prompt, max_new_tokens=5)[0]['generated_text']\n",
        "print(\"Model's answer:\", result)\n"
      ],
      "metadata": {
        "id": "LxIlxeSiUWg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U datasets\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Loading FLAN-T5 model and tokenizer\n",
        "model_name = \"google/flan-t5-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Set up inference pipeline\n",
        "pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "#load dataset with a smaller split for testing\n",
        "dataset = load_dataset(\"commonsense_qa\", split=\"validation[:50]\")\n",
        "\n",
        "\n",
        "import random\n",
        "\n",
        "# using 2 questions from the training set\n",
        "context_examples = [\n",
        "  {\n",
        "    \"question\": \"Why would someone wear gloves?\",\n",
        "    \"choices\": [\"To keep their hands clean\", \"To make their hands soft\", \"To show off\", \"To cover scars\"],\n",
        "    \"answer\": \"A\"\n",
        "  },\n",
        "  {\n",
        "    \"question\": \"Why do people use umbrellas?\",\n",
        "    \"choices\": [\"To fly\", \"To block the sun or rain\", \"To walk faster\", \"To play with\"],\n",
        "    \"answer\": \"B\"\n",
        "  }\n",
        "]\n",
        "\n",
        "# mapping the letter to create a functions of each letter\n",
        "letter_map = [\"A\",\"B\",\"C\",\"D\",\"E\"]\n",
        "\n",
        "# defining variables to keep track of the score\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "for sample in dataset:\n",
        "    question = sample[\"question\"]\n",
        "    choices = sample[\"choices\"][\"text\"]\n",
        "    correct_answer_idx = sample[\"answerKey\"]\n",
        "    correct_answer = correct_answer_idx\n",
        "\n",
        "    #Building the in-context prompt\n",
        "    prompt = \"\"\n",
        "    for ex in context_examples:\n",
        "        ex_q = ex[\"question\"]\n",
        "        ex_choices = ex[\"choices\"]\n",
        "        ex_ans = ex[\"answer\"]\n",
        "        prompt += f\"Question: {ex_q}\\n\"\n",
        "        for i, choice in enumerate(ex_choices):\n",
        "            prompt += f\"{letter_map[i]}) {choice}\\n\"\n",
        "        prompt += f\"Answer: {ex_ans}\\n\\n\"\n",
        "\n",
        "\n",
        "    prompt += f\"Question: {question}\\n\"\n",
        "    for i, choice in enumerate(choices):\n",
        "        prompt += f\"{letter_map[i]}) {choice}\\n\"\n",
        "    prompt += \"Answer:\"\n",
        "\n",
        "    # generating the answer\n",
        "    result = pipe(prompt, max_new_tokens=5, do_sample=False)[0]['generated_text'].strip()\n",
        "    prediction = result[0] if result and result[0] in letter_map else \"?\"\n",
        "\n",
        "    # comparing with the truth\n",
        "    is_correct = (prediction == correct_answer)\n",
        "    correct += int(is_correct)\n",
        "    total += 1\n",
        "\n",
        "    print(f\"Q: {question}\")\n",
        "    print(f\"Predicted: {prediction}, Actual: {correct_answer}, Correct: {is_correct}\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f\"\\nFinal Accuracy over {total} examples: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "tvWjrwuGwzLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U datasets\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Loading FLAN-T5 model and tokenizer\n",
        "model_name = \"google/flan-t5-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Set up inference pipeline\n",
        "pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "#load dataset with a smaller split for testing\n",
        "dataset = load_dataset(\"commonsense_qa\", split=\"validation[:50]\")\n",
        "\n",
        "\n",
        "import random\n",
        "\n",
        "# using 2 questions from the training set\n",
        "context_examples = [\n",
        "  {\n",
        "    \"question\": \"Why would someone wear gloves?\",\n",
        "    \"choices\": [\"To keep their hands clean\", \"To make their hands soft\", \"To show off\", \"To cover scars\"],\n",
        "    \"answer\": \"A\"\n",
        "  },\n",
        "  {\n",
        "    \"question\": \"Why do people use umbrellas?\",\n",
        "    \"choices\": [\"To fly\", \"To block the sun or rain\", \"To walk faster\", \"To play with\"],\n",
        "    \"answer\": \"B\"\n",
        "  }\n",
        "]\n",
        "\n",
        "# mapping the letter to create a functions of each letter\n",
        "letter_map = [\"A\",\"B\",\"C\",\"D\",\"E\"]\n",
        "\n",
        "# defining variables to keep track of the score\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "for sample in dataset:\n",
        "    question = sample[\"question\"]\n",
        "    choices = sample[\"choices\"][\"text\"]\n",
        "    correct_answer_idx = sample[\"answerKey\"]\n",
        "    correct_answer = correct_answer_idx\n",
        "\n",
        "    #Building the in-context prompt\n",
        "    prompt = \"\"\n",
        "    for ex in context_examples:\n",
        "        ex_q = ex[\"question\"]\n",
        "        ex_choices = ex[\"choices\"]\n",
        "        ex_ans = ex[\"answer\"]\n",
        "        prompt += f\"Question: {ex_q}\\n\"\n",
        "        for i, choice in enumerate(ex_choices):\n",
        "            prompt += f\"{letter_map[i]}) {choice}\\n\"\n",
        "        prompt += f\"Answer: {ex_ans}\\n\\n\"\n",
        "\n",
        "\n",
        "    prompt += f\"Question: {question}\\n\"\n",
        "    for i, choice in enumerate(choices):\n",
        "        prompt += f\"{letter_map[i]}) {choice}\\n\"\n",
        "    prompt += \"Answer:\"\n",
        "\n",
        "    # generating the answer\n",
        "    result = pipe(prompt, max_new_tokens=5, do_sample=False)[0]['generated_text'].strip()\n",
        "    prediction = result[0] if result and result[0] in letter_map else \"?\"\n",
        "\n",
        "    # comparing with the truth\n",
        "    is_correct = (prediction == correct_answer)\n",
        "    correct += int(is_correct)\n",
        "    total += 1\n",
        "\n",
        "    print(f\"Q: {question}\")\n",
        "    print(f\"Predicted: {prediction}, Actual: {correct_answer}, Correct: {is_correct}\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f\"\\nFinal Accuracy over {total} examples: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "56H9tW1oVqTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U datasets\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Loading FLAN-T5 model and tokenizer\n",
        "model_name = \"google/flan-t5-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Set up inference pipeline\n",
        "pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "#load dataset with a smaller split for testing\n",
        "dataset = load_dataset(\"commonsense_qa\", split=\"validation[:50]\")\n",
        "\n",
        "import csv\n",
        "import os\n",
        "\n",
        "# Create a folder to store results (optional)\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "\n",
        "# **Define `context_examples` and `letter_map` here, before they are used:**\n",
        "context_examples = [\n",
        "  {\n",
        "    \"question\": \"Why would someone wear gloves?\",\n",
        "    \"choices\": [\"To keep their hands clean\", \"To make their hands soft\", \"To show off\", \"To cover scars\"],\n",
        "    \"answer\": \"A\"\n",
        "  },\n",
        "  {\n",
        "    \"question\": \"Why do people use umbrellas?\",\n",
        "    \"choices\": [\"To fly\", \"To block the sun or rain\", \"To walk faster\", \"To play with\"],\n",
        "    \"answer\": \"B\"\n",
        "  }\n",
        "]\n",
        "\n",
        "letter_map = [\"A\",\"B\",\"C\",\"D\",\"E\"]\n",
        "!pip install -U datasets\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Loading FLAN-T5 model and tokenizer\n",
        "model_name = \"google/flan-t5-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Set up inference pipeline\n",
        "pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "#load dataset with a smaller split for testing\n",
        "dataset = load_dataset(\"commonsense_qa\", split=\"validation[:50]\")\n",
        "\n",
        "import csv\n",
        "import os\n",
        "\n",
        "# Create a folder to store results (optional)\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "\n",
        "# **Define `context_examples` and `letter_map` here, before they are used:**\n",
        "context_examples = [\n",
        "  {\n",
        "    \"question\": \"Why would someone wear gloves?\",\n",
        "    \"choices\": [\"To keep their hands clean\", \"To make their hands soft\", \"To show off\", \"To cover scars\"],\n",
        "    \"answer\": \"A\"\n",
        "  },\n",
        "  {\n",
        "    \"question\": \"Why do people use umbrellas?\",\n",
        "    \"choices\": [\"To fly\", \"To block the sun or rain\", \"To walk faster\", \"To play with\"],\n",
        "    \"answer\": \"B\"\n",
        "  }\n",
        "]\n",
        "\n",
        "letter_map = [\"A\",\"B\",\"C\",\"D\",\"E\"]\n",
        "\n",
        "# Open a CSV file to save results\n",
        "with open(\"C:\\Users\\chris\\Downloads\\results\\icl_results.csvv\", mode=\"w\", newline=\"\") as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"Question\", \"Choices\", \"GroundTruth\", \"Prediction\", \"Correct\"])\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for sample in dataset:\n",
        "        question = sample[\"question\"]\n",
        "        choices = sample[\"choices\"][\"text\"]\n",
        "        correct_answer_idx = sample[\"answerKey\"]\n",
        "        correct_answer = correct_answer_idx\n",
        "\n",
        "        # Build in-context prompt\n",
        "        prompt = \"\"\n",
        "        for ex in context_examples:\n",
        "            ex_q = ex[\"question\"]\n",
        "            ex_choices = ex[\"choices\"]\n",
        "            ex_ans = ex[\"answer\"]\n",
        "            prompt += f\"Question: {ex_q}\\n\"\n",
        "            for i, c in enumerate(ex_choices):\n",
        "                prompt += f\"{letter_map[i]}) {c}\\n\"\n",
        "            prompt += f\"Answer: {ex_ans}\\n\\n\"\n",
        "\n",
        "        prompt += f\"Question: {question}\\n\"\n",
        "        for i, c in enumerate(choices):\n",
        "            prompt += f\"{letter_map[i]}) {c}\\n\"\n",
        "        prompt += \"Answer:\"\n",
        "\n",
        "        # Generate answer\n",
        "        result = pipe(prompt, max_new_tokens=5, do_sample=False)[0]['generated_text'].strip()\n",
        "        prediction = result[0] if result and result[0] in letter_map else \"?\"\n",
        "\n",
        "        is_correct = (prediction == correct_answer)\n",
        "        correct += int(is_correct)\n",
        "        total += 1\n",
        "\n",
        "        # Write row to CSV\n",
        "        writer.writerow([\n",
        "            question,\n",
        "            \" | \".join(choices),\n",
        "            correct_answer,\n",
        "            prediction,\n",
        "            \"Yes\" if is_correct else \"No\"\n",
        "        ])\n",
        "\n",
        "        print(f\"Processed Q{total}: {prediction} vs. {correct_answer} — {'Correct' if is_correct else 'Wrong'}\")\n",
        "\n",
        "# Final accuracy\n",
        "accuracy = correct / total\n",
        "print(f\"\\nFinal Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for sample in dataset:\n",
        "        question = sample[\"question\"]\n",
        "        choices = sample[\"choices\"][\"text\"]\n",
        "        correct_answer_idx = sample[\"answerKey\"]\n",
        "        correct_answer = correct_answer_idx\n",
        "\n",
        "        # Build in-context prompt\n",
        "        prompt = \"\"\n",
        "        for ex in context_examples:\n",
        "            ex_q = ex[\"question\"]\n",
        "            ex_choices = ex[\"choices\"]\n",
        "            ex_ans = ex[\"answer\"]\n",
        "            prompt += f\"Question: {ex_q}\\n\"\n",
        "            for i, c in enumerate(ex_choices):\n",
        "                prompt += f\"{letter_map[i]}) {c}\\n\"\n",
        "            prompt += f\"Answer: {ex_ans}\\n\\n\"\n",
        "\n",
        "        prompt += f\"Question: {question}\\n\"\n",
        "        for i, c in enumerate(choices):\n",
        "            prompt += f\"{letter_map[i]}) {c}\\n\"\n",
        "        prompt += \"Answer:\"\n",
        "\n",
        "        # Generate answer\n",
        "        result = pipe(prompt, max_new_tokens=5, do_sample=False)[0]['generated_text'].strip()\n",
        "        prediction = result[0] if result and result[0] in letter_map else \"?\"\n",
        "\n",
        "        is_correct = (prediction == correct_answer)\n",
        "        correct += int(is_correct)\n",
        "        total += 1\n",
        "\n",
        "        # Write row to CSV\n",
        "        writer.writerow([\n",
        "            question,\n",
        "            \" | \".join(choices),\n",
        "            correct_answer,\n",
        "            prediction,\n",
        "            \"Yes\" if is_correct else \"No\"\n",
        "        ])\n",
        "\n",
        "        print(f\"Processed Q{total}: {prediction} vs. {correct_answer} — {'Correct' if is_correct else 'Wrong'}\")\n",
        "\n",
        "# Final accuracy\n",
        "accuracy = correct / total\n",
        "print(f\"\\nFinal Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "Ei2M00k2M7b3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}